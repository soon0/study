{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB03 Liner Regression and How to minimize cost \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1,2,3])\n",
    "Y = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000 |   74.66667\n",
      "-2.429 |   54.85714\n",
      "-1.857 |   38.09524\n",
      "-1.286 |   24.38095\n",
      "-0.714 |   13.71429\n",
      "-0.143 |    6.09524\n",
      " 0.429 |    1.52381\n",
      " 1.000 |    0.00000\n",
      " 1.571 |    1.52381\n",
      " 2.143 |    6.09524\n",
      " 2.714 |   13.71429\n",
      " 3.286 |   24.38095\n",
      " 3.857 |   38.09524\n",
      " 4.429 |   54.85714\n",
      " 5.000 |   74.66667\n"
     ]
    }
   ],
   "source": [
    "# python 코드로 cost function 구현\n",
    "\n",
    "def cost_func(W,X,Y):\n",
    "    c = 0\n",
    "    for i in range(len(X)):\n",
    "        c += (W*X[i]-Y[i])**2\n",
    "    return c/len(X)\n",
    "\n",
    "for feed_W in np.linspace(-3,5,num=15):\n",
    "    curr_cost = cost_func(feed_W,X,Y)\n",
    "    print(\"{:6.3f} | {:10.5f}\".format(feed_W, curr_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000 |   74.66667\n",
      "-2.429 |   54.85714\n",
      "-1.857 |   38.09524\n",
      "-1.286 |   24.38095\n",
      "-0.714 |   13.71429\n",
      "-0.143 |    6.09524\n",
      " 0.429 |    1.52381\n",
      " 1.000 |    0.00000\n",
      " 1.571 |    1.52381\n",
      " 2.143 |    6.09524\n",
      " 2.714 |   13.71429\n",
      " 3.286 |   24.38095\n",
      " 3.857 |   38.09524\n",
      " 4.429 |   54.85714\n",
      " 5.000 |   74.66667\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow로 cost\n",
    "\n",
    "def cost_func_ten(W,X,Y):\n",
    "    hypothesis = X*W\n",
    "    return tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "\n",
    "W_values = np.linspace(-3,5,num=15)\n",
    "cost_values = []\n",
    "\n",
    "for feed_W in W_values:\n",
    "    curr_cost = cost_func(feed_W,X,Y)\n",
    "    cost_values.append(curr_cost)\n",
    "    print(\"{:6.3f} | {:10.5f}\".format(feed_W, curr_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent\n",
    "\n",
    "tf.set_random_seed(0)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |    74.6667 |   4.813334\n",
      "   10 |    28.7093 |   3.364572\n",
      "   20 |    11.0387 |   2.466224\n",
      "   30 |     4.2444 |   1.909177\n",
      "   40 |     1.6320 |   1.563762\n",
      "   50 |     0.6275 |   1.349578\n",
      "   60 |     0.2413 |   1.216766\n",
      "   70 |     0.0928 |   1.134412\n",
      "   80 |     0.0357 |   1.083346\n",
      "   90 |     0.0137 |   1.051681\n",
      "  100 |     0.0053 |   1.032047\n",
      "  110 |     0.0020 |   1.019871\n",
      "  120 |     0.0008 |   1.012322\n",
      "  130 |     0.0003 |   1.007641\n",
      "  140 |     0.0001 |   1.004738\n",
      "  150 |     0.0000 |   1.002938\n",
      "  160 |     0.0000 |   1.001822\n",
      "  170 |     0.0000 |   1.001130\n",
      "  180 |     0.0000 |   1.000700\n",
      "  190 |     0.0000 |   1.000434\n",
      "  200 |     0.0000 |   1.000269\n",
      "  210 |     0.0000 |   1.000167\n",
      "  220 |     0.0000 |   1.000103\n",
      "  230 |     0.0000 |   1.000064\n",
      "  240 |     0.0000 |   1.000040\n",
      "  250 |     0.0000 |   1.000025\n",
      "  260 |     0.0000 |   1.000015\n",
      "  270 |     0.0000 |   1.000009\n",
      "  280 |     0.0000 |   1.000006\n",
      "  290 |     0.0000 |   1.000004\n"
     ]
    }
   ],
   "source": [
    "x_data = [1.,2.,3.,4.]\n",
    "y_data = [1.,3.,5.,7.]\n",
    "\n",
    "# W = tf.Variable(tf.random_normal([1],-100.,100.))\n",
    "W = tf.Variable([5.0])\n",
    "\n",
    "for step in range(300):\n",
    "    hypothesis = W*X\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "    \n",
    "    alpha = 0.01 # 학습률\n",
    "    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W,X)-Y, X))\n",
    "    descent = W - tf.multiply(alpha, gradient) # 새로운 W\n",
    "    W.assign(descent)\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print('{:5} | {:10.4f} | {:10.6f}'.format(\n",
    "            step, cost.numpy(), W.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab04 Multi variable Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([-9.770107], dtype=float32)> \n",
      " <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([5.9177895], dtype=float32)> \n",
      " <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([3.0865002], dtype=float32)>\n",
      "    0 | 834.989624 |    -9.7701 |     5.9178 |   3.086500\n",
      "   50 | 231.068451 |    -4.7762 |     3.9462 |   3.717954\n",
      "  100 |  71.854523 |    -2.3516 |     2.5926 |   3.867764\n",
      "  150 |  25.661224 |    -1.1579 |     1.6830 |   3.833462\n",
      "  200 |  10.763614 |    -0.5586 |     1.0814 |   3.739948\n",
      "  250 |   5.464667 |    -0.2492 |     0.6887 |   3.636357\n",
      "  300 |   3.420655 |    -0.0833 |     0.4360 |   3.539634\n",
      "  350 |   2.574336 |     0.0105 |     0.2759 |   3.453777\n",
      "  400 |   2.194760 |     0.0673 |     0.1767 |   3.378110\n",
      "  450 |   2.004071 |     0.1046 |     0.1174 |   3.310659\n",
      "  500 |   1.891934 |     0.1313 |     0.0840 |   3.249425\n",
      "  550 |   1.813376 |     0.1522 |     0.0675 |   3.192772\n",
      "  600 |   1.749785 |     0.1696 |     0.0619 |   3.139481\n",
      "  650 |   1.693382 |     0.1849 |     0.0633 |   3.088681\n",
      "  700 |   1.640887 |     0.1990 |     0.0692 |   3.039772\n",
      "  750 |   1.590898 |     0.2122 |     0.0779 |   2.992345\n",
      "  800 |   1.542801 |     0.2248 |     0.0884 |   2.946122\n",
      "  850 |   1.496314 |     0.2369 |     0.0999 |   2.900918\n",
      "  900 |   1.451293 |     0.2488 |     0.1121 |   2.856606\n",
      "  950 |   1.407654 |     0.2603 |     0.1246 |   2.813100\n",
      " 1000 |   1.365338 |     0.2716 |     0.1372 |   2.770340\n"
     ]
    }
   ],
   "source": [
    "x1_data = [1, 0, 3, 0, 5]\n",
    "x2_data = [0, 2, 0, 4, 0]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([1], -10.0, 10.0))\n",
    "W2 = tf.Variable(tf.random_uniform([1], -10.0, 10.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -10.0, 10.0))\n",
    "\n",
    "print(W1,'\\n',W2,'\\n',b)\n",
    "\n",
    "learning_rate = tf.Variable(0.001)\n",
    "\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = W1 * x1_data + W2 * x2_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\"\n",
    "             .format(i, cost.numpy(), W1.numpy()[0], W2.numpy()[0], b.numpy()[0]))\n",
    "        \n",
    "    W1_grad, W2_grad, b_grad = tape.gradient(cost,[W1,W2,b])\n",
    "    W1.assign_sub(learning_rate * W1_grad)\n",
    "    W2.assign_sub(learning_rate * W2_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |   9.845419 |    -0.0378 |     0.4509 |  -0.156377\n",
      "   50 |   2.299131 |     0.4923 |     0.6369 |   0.036622\n",
      "  100 |   0.567490 |     0.7369 |     0.7479 |   0.131253\n",
      "  150 |   0.155373 |     0.8493 |     0.8156 |   0.177673\n",
      "  200 |   0.051269 |     0.9006 |     0.8576 |   0.200136\n",
      "  250 |   0.022546 |     0.9240 |     0.8843 |   0.210495\n",
      "  300 |   0.013650 |     0.9347 |     0.9014 |   0.214621\n",
      "  350 |   0.010489 |     0.9397 |     0.9127 |   0.215472\n",
      "  400 |   0.009172 |     0.9422 |     0.9202 |   0.214568\n",
      "  450 |   0.008509 |     0.9437 |     0.9254 |   0.212708\n",
      "  500 |   0.008098 |     0.9447 |     0.9291 |   0.210322\n",
      "  550 |   0.007791 |     0.9455 |     0.9318 |   0.207647\n",
      "  600 |   0.007531 |     0.9463 |     0.9339 |   0.204819\n",
      "  650 |   0.007294 |     0.9470 |     0.9356 |   0.201916\n",
      "  700 |   0.007070 |     0.9478 |     0.9370 |   0.198984\n",
      "  750 |   0.006855 |     0.9485 |     0.9383 |   0.196050\n",
      "  800 |   0.006648 |     0.9493 |     0.9394 |   0.193133\n",
      "  850 |   0.006448 |     0.9500 |     0.9404 |   0.190241\n",
      "  900 |   0.006254 |     0.9508 |     0.9414 |   0.187381\n",
      "  950 |   0.006066 |     0.9515 |     0.9423 |   0.184557\n",
      " 1000 |   0.005884 |     0.9522 |     0.9432 |   0.181772\n"
     ]
    }
   ],
   "source": [
    "x_data = [\n",
    "    [1., 0., 3., 0., 5.],\n",
    "    [0., 2., 0., 4., 0.]\n",
    "]\n",
    "y_data = [1, 2, 3, 4, 5]\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1,2], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "learning_rate = tf.Variable(0.001)\n",
    "\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, x_data) + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "        \n",
    "        W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "        W.assign_sub(learning_rate * W_grad)\n",
    "        b.assign_sub(learning_rate * b_grad)\n",
    "        \n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\"\n",
    "             .format(i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], b.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |  23.365704 |    -0.9046 |    -0.3252 |     0.0905\n",
      "   50 |   5.799403 |    -0.5839 |     0.4377 |     0.4627\n",
      "  100 |   1.559484 |    -0.4180 |     0.7851 |     0.6888\n",
      "  150 |   0.466727 |    -0.3294 |     0.9413 |     0.8282\n",
      "  200 |   0.158525 |    -0.2804 |     1.0101 |     0.9149\n",
      "  250 |   0.062023 |    -0.2519 |     1.0392 |     0.9693\n",
      "  300 |   0.028606 |    -0.2343 |     1.0508 |     1.0035\n",
      "  350 |   0.016023 |    -0.2228 |     1.0547 |     1.0249\n",
      "  400 |   0.010953 |    -0.2148 |     1.0554 |     1.0384\n",
      "  450 |   0.008769 |    -0.2087 |     1.0549 |     1.0467\n",
      "  500 |   0.007743 |    -0.2038 |     1.0539 |     1.0517\n",
      "  550 |   0.007194 |    -0.1996 |     1.0528 |     1.0546\n",
      "  600 |   0.006845 |    -0.1959 |     1.0518 |     1.0561\n",
      "  650 |   0.006584 |    -0.1925 |     1.0508 |     1.0568\n",
      "  700 |   0.006363 |    -0.1893 |     1.0499 |     1.0570\n",
      "  750 |   0.006162 |    -0.1863 |     1.0491 |     1.0567\n",
      "  800 |   0.005973 |    -0.1833 |     1.0483 |     1.0563\n",
      "  850 |   0.005792 |    -0.1805 |     1.0475 |     1.0557\n",
      "  900 |   0.005617 |    -0.1777 |     1.0467 |     1.0551\n",
      "  950 |   0.005448 |    -0.1750 |     1.0460 |     1.0543\n",
      " 1000 |   0.005284 |    -0.1723 |     1.0453 |     1.0536\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# bias 추가\n",
    "\n",
    "x_data = [\n",
    "    [1., 1., 1., 1., 1.], # bias(b)\n",
    "    [1., 0., 3., 0., 5.], \n",
    "    [0., 2., 0., 4., 0.]\n",
    "]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1, 3], -1.0, 1.0))\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, x_data) # b가 없다\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "        \n",
    "    grads = tape.gradient(cost, [W])\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads,[W]))\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.4f}\"\n",
    "              .format(i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], W.numpy()[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.5]\n",
      " [3.5]], shape=(2, 1), dtype=float32)\n",
      "epoch | cost\n",
      "    0 |   0.459601\n",
      "   50 |   0.007049\n",
      "  100 |   0.004820\n",
      "  150 |   0.003295\n",
      "  200 |   0.002253\n",
      "  250 |   0.001540\n",
      "  300 |   0.001053\n",
      "  350 |   0.000720\n",
      "  400 |   0.000492\n",
      "  450 |   0.000337\n",
      "  500 |   0.000230\n",
      "  550 |   0.000157\n",
      "  600 |   0.000108\n",
      "  650 |   0.000074\n",
      "  700 |   0.000050\n",
      "  750 |   0.000034\n",
      "  800 |   0.000024\n",
      "  850 |   0.000016\n",
      "  900 |   0.000011\n",
      "  950 |   0.000008\n",
      " 1000 |   0.000005\n"
     ]
    }
   ],
   "source": [
    "# Custom Gradient\n",
    "\n",
    "X = tf.constant([[1., 2.],\n",
    "                 [3., 4.]])\n",
    "y = tf.constant([[1.5], [3.5]])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]))\n",
    "b = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "\n",
    "n_epoch = 1000+1\n",
    "print(\"epoch | cost\")\n",
    "for i in range(n_epoch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = tf.matmul(X, W) + b\n",
    "        cost = tf.reduce_mean(tf.square(y_pred - y))\n",
    "\n",
    "    grads = tape.gradient(cost, [W, b])\n",
    "    \n",
    "    optimizer.apply_gradients(grads_and_vars = zip(grads, [W,b]))\n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | cost\n",
      "    0 |   213.9912\n",
      "  100 |    15.7712\n",
      "  200 |    15.6649\n",
      "  300 |    15.5834\n",
      "  400 |    15.5023\n",
      "  500 |    15.4216\n",
      "  600 |    15.3414\n",
      "  700 |    15.2616\n",
      "  800 |    15.1822\n",
      "  900 |    15.1034\n",
      " 1000 |    15.0248\n",
      " 1100 |    14.9469\n",
      " 1200 |    14.8693\n",
      " 1300 |    14.7920\n",
      " 1400 |    14.7153\n",
      " 1500 |    14.6388\n",
      " 1600 |    14.5628\n",
      " 1700 |    14.4872\n",
      " 1800 |    14.4120\n",
      " 1900 |    14.3373\n",
      " 2000 |    14.2629\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "    \n",
    "    [ 73., 80.,  75., 152. ],\n",
    "    [ 93., 88.,  93., 185. ],\n",
    "    [ 89., 91.,  90., 180. ],\n",
    "    [ 96., 98., 100., 196. ],\n",
    "    [ 73., 66.,  70., 142. ]\n",
    "], dtype = np.float32)\n",
    "\n",
    "X = data[ :, :-1]\n",
    "y = data[ :, [-1]]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]))\n",
    "b = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "def predict(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "print('epoch | cost')\n",
    "\n",
    "n_epochs = 2000\n",
    "for i in range(n_epochs+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = tf.reduce_mean((tf.square(predict(X)-y)))\n",
    "        \n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    \n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-d4bbe1f3a688>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "tf.matmul(X,W)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[145.62068],\n",
       "       [188.54604],\n",
       "       [178.70964],\n",
       "       [195.95045],\n",
       "       [146.04646]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lab 05 logistic regression eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(777)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train = [[1., 2.],\n",
    "           [2., 3.],\n",
    "           [3., 1.],\n",
    "           [4., 3.],\n",
    "           [5., 3.],\n",
    "           [6., 2.]]\n",
    "y_train = [[0.],\n",
    "           [0.],\n",
    "           [0.],\n",
    "           [1.],\n",
    "           [1.],\n",
    "           [1.],]\n",
    "x_test = [[5., 2.]]\n",
    "y_test = [[[1.]]]\n",
    "\n",
    "x1 = [x[0] for x in x_train]\n",
    "x2 = [x[1] for x in x_train]\n",
    "\n",
    "colors = [int(y[0] % 3) for y in y_train]\n",
    "plt.scatter(x1, x2, c=colors, marker = '^')\n",
    "plt.scatter(x_test[0][0], x_test[0][1], c='red')\n",
    "\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((?, 2), (?, 1)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))#.repeat()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([2,1]), name = 'weight')\n",
    "b = tf.Variable(tf.zeros([1]), name = 'bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형 회귀식에 sigmoid함수를 적용시킨 로지스틱 함수 생성\n",
    "\n",
    "def logistic_regression(features):\n",
    "    hypothesis = tf.div(1., 1. + tf.exp(tf.matmul(features, W) + b))\n",
    "    return hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convex한 loss함수 생성\n",
    "\n",
    "def loss_fn(hypothesis, features, labels):\n",
    "    cost = -tf.reduce_mean(labels * tf.log(logistic_regression(features))\n",
    "                           + (1-labels) * tf.log(1-hypothesis))\n",
    "    return cost\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값을 0.5기준으로 0,1로 리턴\n",
    "\n",
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype = tf.int32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient 값 계산\n",
    "\n",
    "def grad(hypothesis, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(logistic_regression(features), features, labels)\n",
    "    return tape.gradient(loss_value, [W,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lter: 0, Loss: 0.6874\n",
      "lter: 100, Loss: 0.5776\n",
      "lter: 200, Loss: 0.5349\n",
      "lter: 300, Loss: 0.5054\n",
      "lter: 400, Loss: 0.4838\n",
      "lter: 500, Loss: 0.4671\n",
      "lter: 600, Loss: 0.4535\n",
      "lter: 700, Loss: 0.4420\n",
      "lter: 800, Loss: 0.4319\n",
      "lter: 900, Loss: 0.4228\n",
      "lter: 1000, Loss: 0.4144\n",
      "Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1001\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    for features, labels in iter(dataset):\n",
    "        grads = grad(logistic_regression(features), features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars = zip(grads, [W,b]))\n",
    "        if step % 100 == 0:\n",
    "            print(\"lter: {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features), features, labels)))\n",
    "test_acc = accuracy_fn(logistic_regression(x_test), y_test)\n",
    "print('Test Accuracy: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lab 06 softmax classifier eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(777)\n",
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5], [1, 7, 5, 5], [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7]]\n",
      "[[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]\n",
      "[[1. 2. 1. 1.]\n",
      " [2. 1. 3. 2.]\n",
      " [3. 1. 3. 4.]\n",
      " [4. 1. 5. 5.]\n",
      " [1. 7. 5. 5.]\n",
      " [1. 2. 5. 6.]\n",
      " [1. 6. 6. 6.]\n",
      " [1. 7. 7. 7.]]\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "print(X_data)\n",
    "print(y_data)\n",
    "\n",
    "X_data = np.asarray(X_data, dtype=np.float32)\n",
    "y_data = np.asarray(y_data, dtype=np.float32)\n",
    "print(X_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4)\n",
      "(8, 3)\n"
     ]
    }
   ],
   "source": [
    "nb_classes = 3\n",
    "print(X_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'weight:0' shape=(4, 3) dtype=float32, numpy=\n",
      "array([[ 0.7706481 ,  0.37335402, -0.05576323],\n",
      "       [ 0.00358377, -0.5898363 ,  1.5702795 ],\n",
      "       [ 0.2460895 , -0.09918973,  1.4418385 ],\n",
      "       [ 0.3200988 ,  0.526784  , -0.7703731 ]], dtype=float32)>\n",
      "<tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([-1.3080608 , -0.13253094,  0.5513761 ], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "W = tfe.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "b = tfe.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "variables = [W,b]\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.36571955e-02 7.90162385e-03 9.78441238e-01]\n",
      " [3.92597765e-02 1.70347411e-02 9.43705440e-01]\n",
      " [3.80385160e-01 1.67723209e-01 4.51891690e-01]\n",
      " [3.23390663e-01 5.90759777e-02 6.17533386e-01]\n",
      " [3.62997412e-06 6.20727292e-08 9.99996305e-01]\n",
      " [2.62520462e-02 1.07279727e-02 9.63019967e-01]\n",
      " [1.56525111e-05 4.21802781e-07 9.99983907e-01]\n",
      " [2.94076904e-06 3.81133276e-08 9.99997020e-01]], shape=(8, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(tf.matmul(X, W)+ b)\n",
    "print(hypothesis(X_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.9302204  0.06200533 0.00777428]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sample_db = [[8,2,1,4]]\n",
    "sample_db = np.asarray(sample_db, dtype = np.float32)\n",
    "\n",
    "print(hypothesis(sample_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.092351235, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def cost_fn(X,Y):\n",
    "    logits = hypothesis(X)\n",
    "    cost = -tf.reduce_sum(Y * tf.log(logits), axis=1)\n",
    "    cost_mean = tf.reduce_mean(cost)\n",
    "    \n",
    "    return cost_mean\n",
    "\n",
    "print(cost_fn(X_data, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x*x\n",
    "dy_dx = g.gradient(y,x)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=225255, shape=(4, 3), dtype=float32, numpy=\n",
      "array([[ 4.6911864e-03, -1.2276657e-03, -3.4635440e-03],\n",
      "       [ 1.1795112e-03, -5.4611359e-05, -1.1249427e-03],\n",
      "       [-6.9710575e-03,  2.0387769e-03,  4.9322285e-03],\n",
      "       [ 3.5915859e-03, -2.3555383e-03, -1.2360923e-03]], dtype=float32)>, <tf.Tensor: id=225253, shape=(3,), dtype=float32, numpy=array([ 0.00463655,  0.00153696, -0.00617352], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y)\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "print(grad_fn(X_data, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 2.463941\n",
      "Loss at epoch 100: 0.179275\n",
      "Loss at epoch 200: 0.167705\n",
      "Loss at epoch 300: 0.161137\n",
      "Loss at epoch 400: 0.156294\n",
      "Loss at epoch 500: 0.152218\n",
      "Loss at epoch 600: 0.148557\n",
      "Loss at epoch 700: 0.145166\n",
      "Loss at epoch 800: 0.141976\n",
      "Loss at epoch 900: 0.138953\n",
      "Loss at epoch 1000: 0.136075\n",
      "Loss at epoch 1100: 0.133328\n",
      "Loss at epoch 1200: 0.130699\n",
      "Loss at epoch 1300: 0.128182\n",
      "Loss at epoch 1400: 0.125767\n",
      "Loss at epoch 1500: 0.123447\n",
      "Loss at epoch 1600: 0.121218\n",
      "Loss at epoch 1700: 0.119073\n",
      "Loss at epoch 1800: 0.117008\n",
      "Loss at epoch 1900: 0.115019\n",
      "Loss at epoch 2000: 0.113100\n"
     ]
    }
   ],
   "source": [
    "def fit(X, Y, epochs=2000, verbose=100):\n",
    "    optimizer =  tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0) | ((i+1)%verbose==0):\n",
    "            print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))\n",
    "            \n",
    "fit(X_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[6.6792345e-01 3.3174747e-01 3.2914081e-04]\n",
      " [6.6487235e-01 3.3480459e-01 3.2306000e-04]\n",
      " [6.6118670e-01 3.3849722e-01 3.1610331e-04]\n",
      " [4.3210268e-01 5.6781685e-01 8.0482532e-05]\n",
      " [4.5233923e-01 5.4758292e-01 7.7886027e-05]\n",
      " [4.3943655e-01 5.6048602e-01 7.7429206e-05]\n",
      " [8.2723266e-01 1.7270836e-01 5.8971826e-05]\n",
      " [8.4438688e-01 1.5555650e-01 5.6544308e-05]], shape=(8, 3), dtype=float32)\n",
      "tf.Tensor([0 0 0 1 1 1 0 0], shape=(8,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# argmax를 이용하여 하나의 3개중 1개 뽑기!\n",
    "sample_data = [[2,1,3,2]] # answer_label [[0,0,1]]\n",
    "sample_data = np.asarray(sample_data, dtype=np.float32)\n",
    "\n",
    "a = hypothesis(sample_data)\n",
    "\n",
    "print(a)\n",
    "print(tf.argmax(a, 1)) #index: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'weight:0' shape=(4, 3) dtype=float32, numpy=\n",
      "array([[-8.925622  ,  0.80319047,  9.210532  ],\n",
      "       [-1.8268274 , -0.3999761 ,  3.2108247 ],\n",
      "       [ 9.382009  , -2.0781846 , -5.7151337 ],\n",
      "       [-4.1427293 ,  4.0443983 ,  0.17484541]], dtype=float32)>, <tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([-8.072285 , -2.4986632,  9.681714 ], dtype=float32)>]\n",
      "tf.Tensor(\n",
      "[[9.88852378e-09 1.09199718e-04 9.99890804e-01]\n",
      " [3.55720078e-03 6.83649331e-02 9.28077877e-01]\n",
      " [8.36587591e-15 3.37529704e-02 9.66247022e-01]\n",
      " [1.27754396e-11 6.67802155e-01 3.32197845e-01]\n",
      " [1.31160943e-02 9.78965104e-01 7.91884121e-03]\n",
      " [4.51948773e-03 9.95480537e-01 2.35871283e-12]\n",
      " [8.95132005e-01 1.04867980e-01 2.99414573e-08]\n",
      " [9.84694302e-01 1.53057221e-02 9.68957276e-11]], shape=(8, 3), dtype=float32)\n",
      "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n",
      "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "b = hypothesis(X_data)\n",
    "print(variables)\n",
    "print(b)\n",
    "print(tf.argmax(b, 1))\n",
    "print(tf.argmax(y_data, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "When subclassing the `Model` class, you should implement a `call` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-34ae37dfa527>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_classifer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps_per_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1536\u001b[0;31m         validation_split=validation_split)\n\u001b[0m\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     x, y, sample_weights = self._standardize_weights(x, y, sample_weight,\n\u001b[0;32m--> 992\u001b[0;31m                                                      class_weight, batch_size)\n\u001b[0m\u001b[1;32m    993\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_weights\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   1030\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mis_build_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       \u001b[0mdict_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m       \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1244\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbolic_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m       \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_eager_set_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelInputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[0mdummy_input_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m     \u001b[0mdummy_output_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_input_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_symbolic_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_single_as_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \"\"\"\n\u001b[1;32m    834\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m       raise NotImplementedError('When subclassing the `Model` class, you should'\n\u001b[0m\u001b[1;32m    836\u001b[0m                                 ' implement a `call` method.')\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: When subclassing the `Model` class, you should implement a `call` method."
     ]
    }
   ],
   "source": [
    "class softmax_classifer(tf.keras.Model):\n",
    "    def __init__(self, nb_classes):\n",
    "        super(softmax_classifer, self).__init__()\n",
    "        self.W = tfe.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "        self.b = tfe.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "        \n",
    "    def softmax_regression(self, X):\n",
    "        return tf.nn.softmax(tf.matmul(X, self.W)+self.b)\n",
    "    \n",
    "    def cost_fn(self, X, Y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.softmax_regression(X)\n",
    "            cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(logits), axis=1))\n",
    "            \n",
    "            return grads\n",
    "    \n",
    "    def grad_fn(self, X, Y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            cost = self.cost_fn(x_data, y_data)\n",
    "            grads = tape.gradient(cost, self.variables)\n",
    "            \n",
    "            return grads\n",
    "        \n",
    "def fit(self, X, Y, epochs=2000, verbose=500):\n",
    "    optimizer = tf.train.GradientDescentP=Optimizer(learning_rate=0.1)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        grads = self.grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, self.variables))\n",
    "        if (i==0) | ((i+1)%verbose==0):\n",
    "            print('Loss at epoch %d: %f' %(i+1, self.loss_fn(X, Y).numpy()))\n",
    "            \n",
    "model = softmax_classifer(nb_classes)\n",
    "model.fit(X_data, y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "When subclassing the `Model` class, you should implement a `call` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-bc835ba6eacb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_classifer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps_per_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1536\u001b[0;31m         validation_split=validation_split)\n\u001b[0m\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     x, y, sample_weights = self._standardize_weights(x, y, sample_weight,\n\u001b[0;32m--> 992\u001b[0;31m                                                      class_weight, batch_size)\n\u001b[0m\u001b[1;32m    993\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_weights\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   1030\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mis_build_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       \u001b[0mdict_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m       \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1244\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbolic_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m       \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_eager_set_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelInputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[0mdummy_input_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m     \u001b[0mdummy_output_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_input_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_symbolic_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_single_as_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \"\"\"\n\u001b[1;32m    834\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m       raise NotImplementedError('When subclassing the `Model` class, you should'\n\u001b[0m\u001b[1;32m    836\u001b[0m                                 ' implement a `call` method.')\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: When subclassing the `Model` class, you should implement a `call` method."
     ]
    }
   ],
   "source": [
    "class softmax_classifer(tf.keras.Model):\n",
    "    def __init__(self, nb_classes):\n",
    "        super(softmax_classifer, self).__init__()\n",
    "        self.W = tfe.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "        self.b = tfe.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "        \n",
    "    def softmax_regression(self, X):\n",
    "        return tf.nn.softmax(tf.matmul(X, self.W) + self.b)\n",
    "    \n",
    "    def cost_fn(self, X, Y):\n",
    "        logits = self.softmax_regression(X)\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(logits), axis=1))\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def grad_fn(self, X, Y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            cost = self.cost_fn(x_data, y_data)\n",
    "            grads = tape.gradient(cost, self.variables)\n",
    "            \n",
    "            return grads\n",
    "    \n",
    "def fit(self, X, Y, epochs=2000, verbose=500):\n",
    "    optimizer =  tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        grads = self.grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, self.variables))\n",
    "        if (i==0) | ((i+1)%verbose==0):\n",
    "            print('Loss at epoch %d: %f' %(i+1, self.loss_fn(X, Y).numpy()))\n",
    "            \n",
    "model = softmax_classifer(nb_classes)\n",
    "model.fit(X_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 06 Softmax Zoo_classifier-eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(777)  \n",
    "tfe = tf.contrib.eager\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 7)\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32) #tf1.13.1에서는 np.int32, 이전에는 np.float32\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "nb_classes = 7  # 0 ~ 6\n",
    "\n",
    "# Make Y data as onehot shape\n",
    "Y_one_hot = tf.one_hot(list(y_data), nb_classes)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "\n",
    "print(x_data.shape, Y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight and bias setting\n",
    "W = tfe.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
    "b = tfe.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "variables = [W, b]\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "def logit_fn(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "def hypothesis(X): \n",
    "    return tf.nn.softmax(logit_fn(X))\n",
    "\n",
    "def cost_fn(X, Y):\n",
    "    logits = logit_fn(X)\n",
    "    cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                           labels=Y)\n",
    "    cost = tf.reduce_mean(cost_i)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y)\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        return grads\n",
    "    \n",
    "def prediction(X, Y): # 얼마나 잘 맞추는지 확인\n",
    "    pred = tf.argmax(hypothesis(X), 1)\n",
    "    correct_prediction = tf.equal(pred, tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1 Loss: 0.03012450598180294, Acc: 1.0\n",
      "Steps: 100 Loss: 0.02942507341504097, Acc: 1.0\n",
      "Steps: 200 Loss: 0.028751155361533165, Acc: 1.0\n",
      "Steps: 300 Loss: 0.02810770645737648, Acc: 1.0\n",
      "Steps: 400 Loss: 0.027492770925164223, Acc: 1.0\n",
      "Steps: 500 Loss: 0.026904413476586342, Acc: 1.0\n",
      "Steps: 600 Loss: 0.026340961456298828, Acc: 1.0\n",
      "Steps: 700 Loss: 0.025800863280892372, Acc: 1.0\n",
      "Steps: 800 Loss: 0.02528269961476326, Acc: 1.0\n",
      "Steps: 900 Loss: 0.024785133078694344, Acc: 1.0\n",
      "Steps: 1000 Loss: 0.024306969717144966, Acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "def fit(X, Y, epochs=1000, verbose=100):\n",
    "    optimizer =  tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0) | ((i+1)%verbose==0):\n",
    "#             print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))\n",
    "            acc = prediction(X, Y).numpy()\n",
    "            loss = cost_fn(X, Y).numpy() \n",
    "            print('Steps: {} Loss: {}, Acc: {}'.format(i+1, loss, acc))\n",
    "\n",
    "fit(x_data, Y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab 09 XOR eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "plt.scatter(x_data[0][0],x_data[0][1], c='red' , marker='^')\n",
    "plt.scatter(x_data[3][0],x_data[3][1], c='red' , marker='^')\n",
    "plt.scatter(x_data[1][0],x_data[1][1], c='blue' , marker='^')\n",
    "plt.scatter(x_data[2][0],x_data[2][1], c='blue' , marker='^')\n",
    "\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))\n",
    "\n",
    "def preprocess_data(features, labels):\n",
    "    features = tf.cast(features, tf.float32)\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    return features, labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
